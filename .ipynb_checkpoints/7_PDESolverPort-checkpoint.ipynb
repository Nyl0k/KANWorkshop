{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1c6d25-a0c5-483c-8c23-262dc7cf557b",
   "metadata": {},
   "source": [
    "## Port of MATLAB PDE solver\n",
    "\n",
    "Trying to see if I can a Burgers' Equation solver to work in Python for later use with KANs\n",
    "\n",
    "Referencing this page https://www.mathworks.com/help/deeplearning/ug/solve-partial-differential-equations-with-lbfgs-method-and-deep-learning.html\n",
    "\n",
    "Importing a few libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8372ef6-3209-475d-8ea5-73656028afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.autograd.functional as F\n",
    "import torch.nn.functional as Fnn\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db5db19-7138-416b-9fe0-a5d21626dde2",
   "metadata": {},
   "source": [
    "First create some basic evaluation points such as boundary conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a4f065-ae93-4f39-8e31-5d748d2e9108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100]) torch.Size([1, 100]) torch.Size([1, 100])\n"
     ]
    }
   ],
   "source": [
    "#Establish boundary conditions for x, t, u\n",
    "numBoundaryConditionPoints = [25, 25]\n",
    "\n",
    "x0BC1 = -1*torch.ones(1,numBoundaryConditionPoints[0])\n",
    "x0BC2 = torch.ones(1,numBoundaryConditionPoints[1])\n",
    "\n",
    "t0BC1 = torch.linspace(0,1,steps=numBoundaryConditionPoints[0]).reshape(1, -1)\n",
    "t0BC2 = torch.linspace(0,1,steps=numBoundaryConditionPoints[1]).reshape(1, -1)\n",
    "\n",
    "u0BC1 = torch.zeros(1,numBoundaryConditionPoints[0])\n",
    "u0BC2 = torch.zeros(1,numBoundaryConditionPoints[1])\n",
    "\n",
    "#Estabish iniital conditions\n",
    "numInitialConditionPoints  = 50\n",
    "\n",
    "x0IC = torch.linspace(-1,1,steps=numInitialConditionPoints).reshape(1, -1)\n",
    "t0IC = torch.zeros(1,numInitialConditionPoints)\n",
    "u0IC = -torch.sin(torch.pi*x0IC)\n",
    "\n",
    "X0 = torch.cat([x0IC, x0BC1, x0BC2],dim=1)\n",
    "T0 = torch.cat([t0IC, t0BC1, t0BC2],dim=1)\n",
    "U0 = torch.cat([u0IC, u0BC1, u0BC2],dim=1)\n",
    "\n",
    "print(X0.shape, T0.shape, U0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa1006-1c83-4215-bd3a-d6be5b8162f3",
   "metadata": {},
   "source": [
    "Now create some points for it to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1e42d1-f842-4cb5-a298-723eeb82e1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 2])\n",
      "torch.Size([10000])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "numInternalCollocationPoints = 10000;\n",
    "\n",
    "points = torch.randn(numInternalCollocationPoints,2,requires_grad=True)\n",
    "\n",
    "dataX = 2*points[:,0]-1\n",
    "dataT = points[:,1]\n",
    "\n",
    "numBlocks = 8\n",
    "fcOutputSize = 20\n",
    "\n",
    "print(points.shape)\n",
    "print(dataX.shape)\n",
    "print(dataT.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80a67aa-61c1-49d9-ae36-33359fe64eb4",
   "metadata": {},
   "source": [
    "Now create a neural net for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "564f2d90-3e58-4fc2-b416-a5653afd8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(2, 20)\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.fc3 = nn.Linear(20, 20)\n",
    "        self.fc4 = nn.Linear(20, 20)\n",
    "        self.fc5 = nn.Linear(20, 20)\n",
    "        self.fc6 = nn.Linear(20, 20)\n",
    "        self.fc7 = nn.Linear(20, 20)\n",
    "        self.fc8 = nn.Linear(20, 20)\n",
    "        self.fc9 = nn.Linear(20, 1)\n",
    "        self.tanh_n = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.tanh_n(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.tanh_n(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.tanh_n(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.tanh_n(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.tanh_n(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.tanh_n(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.tanh_n(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.tanh_n(x)\n",
    "        x = self.fc9(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3cdaf-899e-48c4-95fd-ae20319a8553",
   "metadata": {},
   "source": [
    "Define loss function (PDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db4bf2d-261b-439f-922a-9776cddccbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelLoss(net,X,T,X0,T0,U0):\n",
    "    #Make predictions with the initial conditions.\n",
    "    XT = torch.cat([X,T], dim=1)\n",
    "    U = net.forward(XT)\n",
    "    \n",
    "    #Calculate derivatives with respect to X and T.\n",
    "    X = X.squeeze()\n",
    "    T = T.squeeze()\n",
    "    U = U.squeeze()\n",
    "\n",
    "    #Calculate Jacobian for Ux and Ut\n",
    "    Ux = torch.autograd.grad(outputs=U, inputs=XT, grad_outputs=torch.ones_like(U), create_graph=True)[0][:,0]\n",
    "    Ut = torch.autograd.grad(outputs=U, inputs=XT, grad_outputs=torch.ones_like(U), create_graph=True)[0][:,1]\n",
    "    \n",
    "    #Calculate second-order derivatives with respect to X (equivalent to trace of Jacobian).\n",
    "    #Ux.requires_grad=True\n",
    "    Uxx = torch.autograd.grad(outputs=Ux, inputs=XT, grad_outputs=torch.ones_like(Ux), create_graph=True)[0][:,0]\n",
    "    \n",
    "    #Calculate mseF. Enforce Burger's equation.\n",
    "    f = Ut + torch.mul(Ux,U) - torch.mul(Uxx,(0.01/torch.pi))\n",
    "    mseF = torch.mean(torch.pow(f,2))\n",
    "    \n",
    "    #Calculate mseU. Enforce initial and boundary conditions.\n",
    "    XT0 = torch.cat([X0,T0],dim=1)\n",
    "    U0Pred = net.forward(XT0)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    mseU = loss_fn(U0Pred,U0)\n",
    "    \n",
    "    #Calculated loss to be minimized by combining errors.\n",
    "    loss = mseF + mseU\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4ec41-5018-4ac8-a7b1-f3e37f68c3a1",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "140a9b14-3ca9-4587-a520-c26047abf239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss=0.25142142176628113\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (epoch % \u001b[32m100\u001b[39m):\n\u001b[32m     24\u001b[39m         loss = closure()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lbfgs.py:457\u001b[39m, in \u001b[36mLBFGS.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_iter != max_iter:\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[32m    455\u001b[39m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m         loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    458\u001b[39m     flat_grad = \u001b[38;5;28mself\u001b[39m._gather_flat_grad()\n\u001b[32m    459\u001b[39m     opt_cond = flat_grad.abs().max() <= tolerance_grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mclosure\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m():\n\u001b[32m     14\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     loss = \u001b[43mmodelLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX0_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT0_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU0_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     loss.backward(retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mmodelLoss\u001b[39m\u001b[34m(net, X, T, X0, T0, U0)\u001b[39m\n\u001b[32m      9\u001b[39m U = U.squeeze()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#Calculate Jacobian for Ux and Ut\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m Ux = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mXT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][:,\u001b[32m0\u001b[39m]\n\u001b[32m     13\u001b[39m Ut = torch.autograd.grad(outputs=U, inputs=XT, grad_outputs=torch.ones_like(U), create_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m][:,\u001b[32m1\u001b[39m]\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#Calculate second-order derivatives with respect to X (equivalent to trace of Jacobian).\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#Ux.requires_grad=True\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:502\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    498\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    499\u001b[39m         grad_outputs_\n\u001b[32m    500\u001b[39m     )\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    513\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    514\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    515\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "\n",
    "X = dataX.unsqueeze(dim=1)\n",
    "T = dataT.unsqueeze(dim=1)\n",
    "X0_ = X0.permute(1, 0)\n",
    "T0_ = T0.permute(1, 0)\n",
    "U0_ = U0.permute(1, 0)\n",
    "epochs = 1500\n",
    "learn_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.LBFGS(net.parameters(), learn_rate)\n",
    "epoch = 0\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = modelLoss(net, X, T, X0_, T0_, U0_)\n",
    "    loss.backward(retain_graph=True)\n",
    "        \n",
    "    return loss\n",
    "\n",
    "for i in range(epochs):\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    if not (epoch % 10):\n",
    "        loss = closure()\n",
    "        print(f\"Epoch: {epoch}, loss={loss.detach()}\")\n",
    "        \n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f3287d-1449-4a55-85f3-71a8982c8834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
